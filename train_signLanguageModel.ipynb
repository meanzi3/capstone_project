{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 수어 인식 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import tensorflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Bidirectional, Dropout, Conv2D, MaxPooling2D, Flatten, Reshape, Conv1D, MaxPooling1D, BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from keras.layers import TimeDistributed\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model (face, pose, left/right hand 인식 가능 모듈)\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.flip(image,1)                      # 이미지 좌/우 반전\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB (OpenCV 영상은 BRG 형식, Mediapipe는 RGB 형식이기 때문에)\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction (result에 detection한 결과 값을 저장)\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR (Mediapipe용으로 RGB로 변환했던 것을 OpenCV 영상처리를 위해 다시 BRG로 되돌림)\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    #mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION) # 얼굴 랜드마크\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) \n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# landmark draw custom (각 connection 마다 다른 DrawingSpec 지정.)\n",
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections 얼굴 랜드마크\n",
    "    \"\"\"\"mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             )\"\"\" \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(132)\n",
    "    #face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(1404) 얼굴 랜드마크\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose,lh, rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join('7WORD_30-150F_90_Data') # 해당 폴더에 저장\n",
    "actions = np.array(['americano','cafelatte','cafemocha']) #,hot','syrup','next'])\n",
    "# actions = np.array(['americano', 'cafelatte', 'cafemocha']) # 아메리카노, 카페모카, 카페라떼\n",
    "no_sequences = 90  #  데이터 수\n",
    "sequence_length = 150 # frame의 길이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        try:\n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#동영상 파일 경로 설정\n",
    "americano_video_path_list = []\n",
    "cafelatte_video_path_list = []\n",
    "cafemocha_video_path_list = []\n",
    "ice_video_path_list = []\n",
    "hot_video_path_list = []\n",
    "syrup_video_path_list = []\n",
    "next_video_path_list = []\n",
    "\n",
    "for i in range(1, 19):\n",
    "    for orientation in ['D', 'F', 'L', 'R', 'U']:\n",
    "        file_name = f'signdata1/zip/NIA_SL_WORD1501_REAL{i:02d}_{orientation}.mp4'\n",
    "        americano_video_path_list.append(file_name)\n",
    "\n",
    "for i in range(1, 19):\n",
    "    for orientation in ['D', 'F', 'L', 'R', 'U']:\n",
    "        file_name = f'signdata1/zip/NIA_SL_WORD1502_REAL{i:02d}_{orientation}.mp4'\n",
    "        cafelatte_video_path_list.append(file_name)\n",
    "\n",
    "for i in range(1, 19):\n",
    "    for orientation in ['D', 'F', 'L', 'R', 'U']:\n",
    "        file_name = f'signdata1/zip/NIA_SL_WORD1503_REAL{i:02d}_{orientation}.mp4'\n",
    "        cafemocha_video_path_list.append(file_name)\n",
    "        \n",
    "for i in range(1, 19):\n",
    "    for orientation in ['D', 'F', 'L', 'R', 'U']:\n",
    "        file_name = f'signdata1/zip/NIA_SL_WORD1504_REAL{i:02d}_{orientation}.mp4'\n",
    "        ice_video_path_list.append(file_name)\n",
    "        \n",
    "for i in range(1, 19):\n",
    "    for orientation in ['D', 'F', 'L', 'R', 'U']:\n",
    "        file_name = f'signdata1/zip/NIA_SL_WORD1505_REAL{i:02d}_{orientation}.mp4'\n",
    "        hot_video_path_list.append(file_name)\n",
    "        \n",
    "for i in range(1, 19):\n",
    "    for orientation in ['D', 'F', 'L', 'R', 'U']:\n",
    "        file_name = f'signdata1/zip/NIA_SL_WORD1506_REAL{i:02d}_{orientation}.mp4'\n",
    "        syrup_video_path_list.append(file_name)\n",
    "        \n",
    "for i in range(1, 19):\n",
    "    for orientation in ['D', 'F', 'L', 'R', 'U']:\n",
    "        file_name = f'signdata1/zip/NIA_SL_WORD1507_REAL{i:02d}_{orientation}.mp4'\n",
    "        next_video_path_list.append(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 수집 코드\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        video_path = action + '_video_path_list[' + str(sequence) + ']'\n",
    "        cap = cv2.VideoCapture(eval(video_path))\n",
    "        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))  # 프레임 높이\n",
    "        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))    # 프레임 너비\n",
    "        start_frame = 30\n",
    "        last_frame = None\n",
    "\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))   #총프레임수 채움\n",
    "        for frame_num in range(sequence_length):\n",
    "            if frame_num >= total_frames:\n",
    "                empty_frame = np.zeros((frame_height, frame_width, 3), dtype=np.uint8)\n",
    "                frame = empty_frame\n",
    "            else:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    if last_frame is not None:\n",
    "                        frame = last_frame.copy()\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "            with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "                if frame_num < start_frame:\n",
    "                    continue\n",
    "\n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "                draw_styled_landmarks(image, results)\n",
    "\n",
    "                if frame_num == 30:\n",
    "                    cv2.putText(image, 'STARTING COLLECTION', (120, 200),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15, 12),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                    cv2.waitKey(2000)\n",
    "                else:\n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15, 12),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "\n",
    "            last_frame = frame\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#레이블 구분\n",
    "label_map = {label:num for num, label in enumerate(actions)}\n",
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#관절 좌표 데이터 로드\n",
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        window = []\n",
    "        for frame_num in range(30, sequence_length):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 확장. 이동/확대/축소 (선택)\n",
    "def random_translation(data, value=0.06):\n",
    "    return data + (np.random.uniform(-value, value, data.shape))\n",
    "def random_scaling(data, scale_range=(0.9, 1.1)):\n",
    "    scale_factor = np.random.uniform(scale_range[0], scale_range[1])\n",
    "    return data * scale_factor\n",
    "augmented_sequences = []\n",
    "augmented_labels = []\n",
    "\n",
    "for sequence, label in zip(sequences, labels):\n",
    "    augmented_sequences.append(sequence)\n",
    "    augmented_labels.append(label)\n",
    "    \n",
    "    # Random Translation\n",
    "    translated_data = random_translation(np.array(sequence))\n",
    "    augmented_sequences.append(translated_data.tolist())\n",
    "    augmented_labels.append(label)\n",
    "\n",
    "    # Random Scaling\n",
    "    scaled_data = random_scaling(np.array(sequence))\n",
    "    augmented_sequences.append(scaled_data.tolist())\n",
    "    augmented_labels.append(label)\n",
    "\n",
    "augmented_sequences = np.array(augmented_sequences)\n",
    "augmented_labels = to_categorical(augmented_labels).astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(augmented_sequences, augmented_labels, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 확장 없이 학습 데이터 생성 \n",
    "np.array(sequences).shape\n",
    "np.array(labels).shape\n",
    "\n",
    "X = np.array(sequences)\n",
    "X.shape\n",
    "\n",
    "y = to_categorical(labels).astype(int)\n",
    "y\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conv1d-lstm-dense 모델\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Conv1D 층\n",
    "model.add(Conv1D(64, 3, activation='relu', padding='same', input_shape=(120, 258)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(2))\n",
    "\n",
    "model.add(Conv1D(128, 3, activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(2))\n",
    "\n",
    "model.add(Conv1D(256, 3, activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(2))\n",
    "\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(2))\n",
    "\n",
    "\n",
    "# LSTM 층\n",
    "#model.add(LSTM(128, return_sequences=True, activation='tanh'))#추가2:0.75\n",
    "#model.add(LSTM(64, return_sequences=True, activation='tanh'))#감소3:0.97\n",
    "model.add(LSTM(32, return_sequences=True, activation='tanh'))\n",
    "model.add(LSTM(16, return_sequences=False, activation='tanh'))#추가:1.0/0,82\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Dense 층\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# 출력 층\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 nan 값 확인\n",
    "has_nan = np.isnan(X_train).any()\n",
    "\n",
    "if has_nan:\n",
    "    print(\"리스트에 NaN 값이 있습니다.\")\n",
    "else:\n",
    "    print(\"리스트에 NaN 값이 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=custom_optimizer, loss = 'categorical_crossentropy' , metrics=['categorical_accuracy'])\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions[np.argmax(res[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions[np.argmax(y_test[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('action.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('action.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#임시 검증\n",
    "yhat = model.predict(X_test)\n",
    "ytrue = np.argmax(y_test, axis=1).tolist()  \n",
    "yhat = np.argmax(yhat, axis=1).tolist()\n",
    "multilabel_confusion_matrix(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#실시간 감지 화면 설정\n",
    "colors = [(245,117,16), (117,245,16), (16,117,245)] #,(255,20,147)] #(255,255,0),(255,0,255),(0,255,255)]\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# 필요한 변수 초기화\n",
    "sequence = []     # 각 프레임의 키포인트를 저장하는 리스트\n",
    "sentence = []     # 인식된 수어를 저장하는 리스트\n",
    "res = np.zeros(len(actions))  # 수어 인식 결과를 저장하는 배열\n",
    "threshold = 0.8   # 적중 확률 임계값\n",
    "start_capture = False  # 수어 인식 시작 여부\n",
    "circle_color = (0, 255, 0)  # 녹화 표시 원의 색 (녹색=대기, 빨간색=녹화중)\n",
    "countdown = 7 # 카운트다운 시간 (초)\n",
    "last_capture_time = time.time()  # 마지막 인식 시작 시간\n",
    "\n",
    "# 웹캠 설정\n",
    "cap = cv2.VideoCapture(0)\n",
    "cv2.namedWindow('OpenCV Feed', cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow('OpenCV Feed', 1920, 1080)\n",
    "\n",
    "# MediaPipe 모델 설정\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        # 현재 시간 업데이트\n",
    "        current_time = time.time()\n",
    "\n",
    "        # 비디오 프레임 읽기\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # 랜드마크 감지\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "        # 카운트다운 표시\n",
    "        if not start_capture and current_time - last_capture_time < countdown:\n",
    "            time_left = countdown - int(current_time - last_capture_time)\n",
    "            cv2.putText(image, str(time_left), (320, 240),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 255), 3, cv2.LINE_AA)\n",
    "        elif not start_capture:\n",
    "            start_capture = True\n",
    "\n",
    "        # 랜드마크 그리기\n",
    "        draw_styled_landmarks(image, results)\n",
    "\n",
    "        # 수어 인식 시작\n",
    "        if start_capture:\n",
    "            keypoints = extract_keypoints(results)\n",
    "            sequence.append(keypoints)\n",
    "            circle_color = (0, 0, 255)  # 빨간 원으로 변경 (녹화 중)\n",
    "\n",
    "            # \"next\" 수어가 인식되면 프로그램 종료\n",
    "            #if \"next\" in sentence:\n",
    "                #break\n",
    "\n",
    "            # 시퀀스 길이가 120에 도달하면 수어 인식 시작\n",
    "            if len(sequence) == 120:\n",
    "                res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "                print(actions[np.argmax(res)])  # 인식된 수어 출력\n",
    "\n",
    "                # 인식된 수어 처리\n",
    "                if res[np.argmax(res)] > threshold:\n",
    "                    if len(sentence) > 0 and actions[np.argmax(res)] != sentence[-1]:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "                    elif len(sentence) == 0:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "                    \n",
    "\n",
    "                # 다음 인식을 위한 초기화\n",
    "                start_capture = False\n",
    "                last_capture_time = time.time()\n",
    "                sequence = []\n",
    "\n",
    "        # 인식 결과 시각화\n",
    "        image = prob_viz(res, actions, image, colors)\n",
    "        cv2.circle(image, (610, 450), 20, circle_color, -1)\n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3,30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        # 화면에 표시\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # 키 입력 처리 ('q'로 종료)\n",
    "        key = cv2.waitKey(10)\n",
    "        if key & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # 웹캠 및 OpenCV 창 해제\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
